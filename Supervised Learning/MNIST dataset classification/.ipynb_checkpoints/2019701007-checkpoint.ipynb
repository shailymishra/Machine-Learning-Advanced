{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Assignment 1\n",
    "This jupyter notebook is meant to be used in conjunction with the full questions in the assignment pdf.\n",
    "\n",
    "## Instructions\n",
    "- Write your code and analyses in the indicated cells.\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Do not attempt to change the contents of the other cells.\n",
    "\n",
    "## Submission\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Rename the notebook to `<roll_number>.ipynb` and submit ONLY the notebook file on moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Environment setup\n",
    "\n",
    "The following code reads the train and test data (provided along with this template) and outputs the data and labels as numpy arrays. Use these variables in your code.\n",
    "\n",
    "---\n",
    "#### Note on conventions\n",
    "In mathematical notation, the convention is tha data matrices are column-indexed, which means that a input data $x$ has shape $[d, n]$, where $d$ is the number of dimensions and $n$ is the number of data points, respectively.\n",
    "\n",
    "Programming languages have a slightly different convention. Data matrices are of shape $[n, d]$. This has the benefit of being able to access the ith data point as a simple `data[i]`.\n",
    "\n",
    "What this means is that you need to be careful about your handling of matrix dimensions. For example, while the covariance matrix (of shape $[d,d]$) for input data $x$ is calculated as $(x-u)(x-u)^T$, while programming you would do $(x-u)^T(x-u)$ to get the correct output shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    num_points = len(lines)\n",
    "    dim_points = 28 * 28\n",
    "    data = np.empty((num_points, dim_points))\n",
    "    labels = np.empty(num_points)\n",
    "    \n",
    "    for ind, line in enumerate(lines):\n",
    "        num = line.split(',')\n",
    "        labels[ind] = int(num[0])\n",
    "        data[ind] = [ int(x) for x in num[1:] ]\n",
    "        \n",
    "    return (data, labels)\n",
    "\n",
    "train_data, train_labels = read_data(\"sample_train.csv\")\n",
    "test_data, test_labels = read_data(\"sample_test.csv\")\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(train_labels.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Questions\n",
    "---\n",
    "## 1.3.1 Representation\n",
    "The next code cells, when run, should plot the eigen value spectrum of the covariance matrices corresponding to the mentioned samples. Normalize the eigen value spectrum and only show the first 100 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples corresponding to the last digit of your roll number (plot a)\n",
    "# Roll no. 2019701007\n",
    "from numpy import linalg as LA\n",
    "from numpy.linalg import matrix_rank\n",
    "\n",
    "train_sample_of_7 = train_data[train_labels[:]==7]\n",
    "cov = np.cov(train_sample_of_7.T) # need to do transpose to use np.cov\n",
    "# otherwise do it formula wise\n",
    "# mean_vec = np.mean(train_sample_of_7, axis=0)\n",
    "# cov_mat = (train_sample_of_7 - mean_vec).T.dot((train_sample_of_7 - mean_vec)) / (train_sample_of_7.shape[0]-1)\n",
    "eigenvalues, eigenvectors = LA.eigh(cov)\n",
    "eigenvalues.sort()\n",
    "eigenvalues = eigenvalues[::-1]\n",
    "total = sum(eigenvalues)\n",
    "var_exp = [(i / total)*100 for i in eigenvalues]\n",
    "approx_rank_k = len((np.where(eigenvalues>0))[0])\n",
    "print('Approv Rank of Cov matrix',  approx_rank_k )\n",
    "xaxis = np.arange(100)\n",
    "plt.bar(xaxis, var_exp[:100])\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "plt.title('Eigen Spectrum')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples corresponding to the last digit of (your roll number + 1) % 10 (plot b)\n",
    "# for sample no - 8\n",
    "train_sample_of_8 = train_data[train_labels[:]==8]\n",
    "cov = np.cov(train_sample_of_8.T)\n",
    "eigenvalues, eigenvectors = LA.eigh(cov)\n",
    "eigenvalues.sort()\n",
    "eigenvalues = eigenvalues[::-1]\n",
    "total = sum(eigenvalues)\n",
    "var_exp = [(i / total)*100 for i in eigenvalues]\n",
    "approx_rank_k = len((np.where(eigenvalues>0))[0])\n",
    "print('Approv Rank of Cov matrix',  approx_rank_k )\n",
    "xaxis = np.arange(100)\n",
    "plt.bar(xaxis, var_exp[:100])\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "plt.title('Eigen Spectrum')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# All training data (plot c)\n",
    "cov = np.cov(train_data.T)\n",
    "eigenvalues, eigenvectors = LA.eigh(cov)\n",
    "eigenvalues.sort()\n",
    "eigenvalues = eigenvalues[::-1]\n",
    "total = sum(eigenvalues)\n",
    "var_exp = [(i / total)*100 for i in eigenvalues]\n",
    "approx_rank_k = len((np.where(eigenvalues>0))[0])\n",
    "print('Approv Rank of Cov matrix',  approx_rank_k )\n",
    "\n",
    "xaxis = np.arange(100)\n",
    "plt.bar(xaxis, var_exp[:100])\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "plt.title('Eigen Spectrum')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly selected 50% of the training data (plot d)\n",
    "\n",
    "# df = pd.DataFrame(train_data)\n",
    "# half_train_data = df.sample(frac = 0.5) \n",
    "\n",
    "idx = np.random.randint(6000, size=3000)\n",
    "half_train_data = train_data[idx,:]\n",
    "\n",
    "\n",
    "cov = np.cov(half_train_data.T)\n",
    "eigenvalues, eigenvectors = LA.eigh(cov)\n",
    "eigenvalues.sort()\n",
    "eigenvalues = eigenvalues[::-1]\n",
    "total = sum(eigenvalues)\n",
    "var_exp = [(i / total)*100 for i in eigenvalues]\n",
    "approx_rank_k = len((np.where(eigenvalues>0))[0])\n",
    "print('Approv Rank of Cov matrix',  approx_rank_k )\n",
    "\n",
    "xaxis = np.arange(100)\n",
    "plt.bar(xaxis, var_exp[:100])\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "plt.title('Eigen Spectrum')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.1 Question 1\n",
    "- Are plots a and b different? Why?\n",
    "- Are plots b and c different? Why?\n",
    "- What are the approximate ranks of each plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- Plot a is for samples for digit - 7 and Plot b is for samples for digit - 8.\n",
    "Overall the plot has a similiar structure, maximum -> close to zero\n",
    "But there's difference in magnitude and magnitude decrease.\n",
    "Eigen Values correspond to variances across features. so for different digit, variance across differnt features, will be different. \n",
    "Roughly Eigen Values of Plot a goes like 17.5, 12.5, 8, 5,...\n",
    "Roughly Eigen Values of Plot b goes like 14.5, 8, 6, 5,....\n",
    "\n",
    "\n",
    "- Plot b is for samples for digit - 8. Plot c is for samples for all training set.\n",
    "Overall the plot has a similiar structure, maximum -> close to zero\n",
    "But there's much more difference in magnitude and magnitude decrease.\n",
    "Roughly Eigen Values of Plot b goes like 14.5, 8, 6, 5,....\n",
    "Rouhgly Eigen Values of Plot c goes like 10, 7.5, 6.7, 5,...\n",
    "In plot b, the highest eigen value will be more, because its just for a single digit. and hence all sample will have quite similiar pattern,\n",
    "resulting into high varaince.\n",
    "and for plot c, since its all training set - it has all digit samples, it wont have as high variance as plot b. \n",
    "\n",
    "\n",
    "\n",
    "- Plot a : 561\n",
    "  Plot b : 562\n",
    "  Plot c : 704\n",
    "  Plot d : 675\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.1 Question 2\n",
    "- How many possible images could there be?\n",
    "- What percentage is accessible to us as MNIST data?\n",
    "- If we had acces to all the data, how would the eigen value spectrum of the covariance matrix look?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- total 784 dimension, and each dimension can have two values - {0,1} so total possible images 2^784\n",
    "- Percentage : trained data: (6000/(2^784)) * 100, test data : (1000/2^784)) * 100. Quite Quite less. \n",
    "- If we have access to all data, all eigen values will have same value, eigen value spectrum will be constant through. and it makes sense, if we need everything to generate full data, cannot omit anything. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## 1.3.2 Linear Transformation\n",
    "---\n",
    "### 1.3.2 Question 1\n",
    "How does the eigen spectrum change if the original data was multiplied by an orthonormal matrix? Answer analytically and then also validate experimentally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Consider A is our data matrix. For simiplicity consider columns are mean 0. \n",
    "Hence the Covariance Matrix will be (AT)A where AT is A transpose\n",
    "\n",
    "Doing eigen value decomposition of Covariance Matrix : (P D P^-1)\n",
    "\n",
    "P is orthogonal matrix, since it is eigen vectors of symmetric matrix. Covariance is a symmetric matrix.\n",
    "\n",
    "\n",
    "\n",
    "Case 1 : X = AP (Multiply data with Orthogonal matrix on right)\n",
    "New Comvariance Matrix : XTX = (AP)T (AP)\n",
    "= (PT) (AT) (A) (P)\n",
    "= (PT) (ATA) (P)    {now ATA is cov matrix i.e. it is P D P^-1}\n",
    "= (PT) (P D P^-1) (P) {since P is orthogonal PT = P^-1}\n",
    "= D\n",
    "\n",
    "resulting into same set of Eigen Values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Case 2 : X = PA (Multiply data with Orthogonal matrix on left)\n",
    "New Convariance Matrix : XTX = (PA)T (PA)\n",
    "= AT PT P A\n",
    "= AT A {since PT P = I}\n",
    "\n",
    "i.e. same covariance matrix, resulting into same set of Eigen values\n",
    " \n",
    " \n",
    " \n",
    " These were quite generice senarios, multiplying with the orthonomral matrix which is eigen vectors of covariance matrix\n",
    " So overall the eigen value spectrum would be similiar, eigen values may differ\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental validation here.\n",
    "# Multiply your data (train_data) with an orthonormal matrix and plot the\n",
    "# eigen value specturm of the new covariance matrix.\n",
    "\n",
    "# code goes here\n",
    "def rvs(dim):\n",
    "     print('Calculating Orthogonal matrix..........')\n",
    "     random_state = np.random\n",
    "     H = np.eye(dim)\n",
    "     D = np.ones((dim,))\n",
    "     for n in range(1, dim):\n",
    "         x = random_state.normal(size=(dim-n+1,))\n",
    "         D[n-1] = np.sign(x[0])\n",
    "         x[0] -= D[n-1]*np.sqrt((x*x).sum())\n",
    "         # Householder transformation\n",
    "         Hx = (np.eye(dim-n+1) - 2.*np.outer(x, x)/(x*x).sum())\n",
    "         mat = np.eye(dim)\n",
    "         mat[n-1:, n-1:] = Hx\n",
    "         H = np.dot(H, mat)\n",
    "         # Fix the last sign such that the determinant is 1\n",
    "     D[-1] = (-1)**(1-(dim % 2))*D.prod()\n",
    "     # Equivalent to np.dot(np.diag(D), H) but faster, apparently\n",
    "     H = (D*H.T).T\n",
    "     return H\n",
    "\n",
    "\n",
    "cov = np.cov(train_data.T)\n",
    "eigenvalues, eigenvectors = LA.eigh(cov)\n",
    "eigenvalues.sort()\n",
    "eigenvalues = eigenvalues[::-1]\n",
    "total = sum(eigenvalues)\n",
    "var_exp = [(i / total)*100 for i in eigenvalues]\n",
    "approx_rank_k = len((np.where(eigenvalues>0))[0])\n",
    "xaxis1 = np.arange(100)\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.bar(xaxis1, var_exp[:100])\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "plt.title('Eigen Spectrum of original data')\n",
    "\n",
    "# Case = train_data x orthono\n",
    "orthogonalmatrix1 =  rvs(784)\n",
    "new_cov = np.cov(train_data.T)\n",
    "new_data = train_data.dot(orthogonalmatrix1)\n",
    "new_eigenvalues, new_eigenvectors = LA.eigh(new_cov)\n",
    "new_eigenvalues.sort()\n",
    "new_eigenvalues = new_eigenvalues[::-1]\n",
    "new_total = sum(new_eigenvalues)\n",
    "new_var_exp = [(i / new_total)*100 for i in new_eigenvalues]\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "xaxis1 = np.arange(100)\n",
    "plt.bar(xaxis, new_var_exp[:100])\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "plt.title('Eigen Spectrum of new data')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.2 Question 2\n",
    "If  samples  were  multiplied  by  784 × 784  matrix  of rank 1 or 2, (rank deficient matrices), how will the eigen spectrum look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a matrix is Rank 1, that means it only has one non zero eigen value.  \n",
    "And when we multiply a matrix with rank one matrix, it also becomes rank1.\n",
    "Proof :\n",
    "Rank1matrix = Column x row = uvT\n",
    "\n",
    "if a matrix A is multiplied by Rank1matrix, so it becomes AuvT. now Au is again a column. and columnx row is rank1.\n",
    "\n",
    "Basically multiplying the data with rank deficient marix, makes the matrix rank deficient. \n",
    "\n",
    "Hence, lot of columns will be dependent, independent information will be less. and if we calculate the covariance matrix of this matrix, and that will be also rank deficient. \n",
    "\n",
    "\n",
    "Say for A to be rank 1 matrix\n",
    "because it will be ATA = (column*row)T (column * row) = column * row * column * row = scalar * column * row => giving rank1 matrix\n",
    "\n",
    "Eigen specturm, will have a drastic impact. It will lose lot of information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.2 Question 3\n",
    "Project the original data into the first and second eigenvectors and plot in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting code here\n",
    "\n",
    "cov = np.cov(train_data.T) # need to do transpose to use np.cov\n",
    "[eigenvalues, eigenvectors] = LA.eigh(cov)\n",
    "\n",
    "#Get the top two eigen vector\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eigenvalues[i]), eigenvectors[:,i]) for i in range(len(eigenvalues))]\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "matrix_w = np.hstack((eig_pairs[0][1].reshape(784,1), eig_pairs[1][1].reshape(784,1)))\n",
    "transformed = train_data.dot(matrix_w)\n",
    "\n",
    "# then plot\n",
    "plt.plot(transformed[0:6000,0], transformed[0:6000,1], 'o', color='blue',  label='Transformed data')\n",
    "plt.xlim([-3000,1000])\n",
    "plt.ylim([-2000,2000])\n",
    "plt.xlabel('x axis')\n",
    "plt.ylabel('y axis')\n",
    "plt.legend()\n",
    "plt.title('Transformed samples')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## 1.3.3 Probabilistic View\n",
    "---\n",
    "In this section you will classify the test set by fitting multivariate gaussians on the train set, with different choices for decision boundaries. On running, your code should print the accuracy on your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy on the test set using MLE\n",
    "from numpy import diag\n",
    "    \n",
    "def train (train_data, train_labels):\n",
    "    dict_alldata = {};\n",
    "    n_features = len(train_labels)\n",
    "    for i in range(10):\n",
    "        print('Training for sample ', i)\n",
    "        train_sample = train_data[train_labels[:]==i]\n",
    "        cov = np.cov(train_sample.T) \n",
    "        mu = train_sample.mean ( axis=0 )\n",
    "        eigenvalues, eigenvectors = LA.eigh(cov)\n",
    "        eigen = np.column_stack((eigenvalues,eigenvectors))\n",
    "        eigen = eigen[(-eigen[:,0]).argsort()]\n",
    "        sortedEigenValues = np.array(eigen[:,0])\n",
    "        sortedEigenVectors = np.array(eigen[:,1:])\n",
    "        approx_rank_k = len((np.where(sortedEigenValues>0))[0])\n",
    "        first_k_eigen_values = sortedEigenValues[:approx_rank_k]\n",
    "        first_k_eigen_values_inv = np.reciprocal(first_k_eigen_values)\n",
    "        first_k_eigen_vectors = sortedEigenVectors[:approx_rank_k,:]\n",
    "        inverse = first_k_eigen_vectors.T.dot(diag(first_k_eigen_values_inv)).dot(first_k_eigen_vectors)\n",
    "        (sign, logdet) = np.linalg.slogdet(diag(first_k_eigen_values))\n",
    "        Wi = -0.5 * (inverse)\n",
    "        wi = inverse.dot(mu)\n",
    "        wi0 = -0.5*( mu.T.dot(inverse).dot(mu) - logdet )\n",
    "        dict_alldata[i]={\n",
    "            \"Wi\":Wi,\n",
    "            \"wi\":wi,\n",
    "            \"wi0\":wi0\n",
    "        }\n",
    "    return dict_alldata\n",
    "    \n",
    "\n",
    "def classify ( x_test ):\n",
    "        prob = [];\n",
    "        for i in range(10):\n",
    "            Wi = dict_alldata[i][\"Wi\"]\n",
    "            wi = dict_alldata[i][\"wi\"]\n",
    "            wi0 = dict_alldata[i][\"wi0\"]\n",
    "            log_prob = x_test.T.dot(Wi).dot(x_test) + wi.T.dot(x_test) + wi0\n",
    "            prob.append(log_prob)\n",
    "        \n",
    "        maxElement = np.amax(prob)\n",
    "        itemindex = (np.where(prob == np.amax(prob)))\n",
    "        identified_number = itemindex[0][0]\n",
    "        return identified_number\n",
    "\n",
    "dict_alldata = train(train_data,train_labels)\n",
    "accuracy = 0;\n",
    "for index in range(len(test_data)):\n",
    "    identified_number = (classify(test_data[index]))\n",
    "#     print('Identified ',identified_number, 'actually is', test_labels[index])\n",
    "    if(identified_number == test_labels[index]):\n",
    "        accuracy = accuracy + 1\n",
    "        \n",
    "print('Accuracy',(accuracy * 100) / 1000, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy on the test set using MAP\n",
    "# (assume a reasonable prior and mention it in the comments)\n",
    "\n",
    "from numpy import diag\n",
    "\n",
    "# Choosing equally likely prior, because all have same no of samples\n",
    "prior = [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1]\n",
    "\n",
    "def train (train_data, train_labels):\n",
    "    dict_alldata = {};\n",
    "    n_features = len(train_labels)\n",
    "    for i in range(10):\n",
    "        print('Training for sample ', i)\n",
    "        train_sample = train_data[train_labels[:]==i]\n",
    "        cov = np.cov(train_sample.T) \n",
    "        mu = train_sample.mean ( axis=0 )\n",
    "        eigenvalues, eigenvectors = LA.eigh(cov)\n",
    "        eigen = np.column_stack((eigenvalues,eigenvectors))\n",
    "        eigen = eigen[(-eigen[:,0]).argsort()]\n",
    "        sortedEigenValues = np.array(eigen[:,0])\n",
    "        sortedEigenVectors = np.array(eigen[:,1:])\n",
    "        approx_rank_k = len((np.where(sortedEigenValues>0))[0])\n",
    "        first_k_eigen_values = sortedEigenValues[:approx_rank_k]\n",
    "        first_k_eigen_values_inv = np.reciprocal(first_k_eigen_values)\n",
    "        first_k_eigen_vectors = sortedEigenVectors[:approx_rank_k,:]\n",
    "        inverse = first_k_eigen_vectors.T.dot(diag(first_k_eigen_values_inv)).dot(first_k_eigen_vectors)\n",
    "        (sign, logdet) = np.linalg.slogdet(diag(first_k_eigen_values))\n",
    "        Wi = -0.5 * (inverse)\n",
    "        wi = inverse.dot(mu)\n",
    "        wi0 = -0.5*( mu.T.dot(inverse).dot(mu) - logdet )  + np.log(prior[i])\n",
    "        dict_alldata[i]={\n",
    "            \"Wi\":Wi,\n",
    "            \"wi\":wi,\n",
    "            \"wi0\":wi0\n",
    "        }\n",
    "    return dict_alldata\n",
    "    \n",
    "\n",
    "def classify ( x_test ):\n",
    "        prob = [];\n",
    "        for i in range(10):\n",
    "            Wi = dict_alldata[i][\"Wi\"]\n",
    "            wi = dict_alldata[i][\"wi\"]\n",
    "            wi0 = dict_alldata[i][\"wi0\"]\n",
    "            log_prob = x_test.T.dot(Wi).dot(x_test) + wi.T.dot(x_test) + wi0\n",
    "            prob.append(log_prob)\n",
    "        \n",
    "        maxElement = np.amax(prob)\n",
    "        itemindex = (np.where(prob == np.amax(prob)))\n",
    "        identified_number = itemindex[0][0]\n",
    "        return identified_number\n",
    "\n",
    "dict_alldata = train(train_data,train_labels)\n",
    "accuracy = 0;\n",
    "for index in range(len(test_data)):\n",
    "    identified_number = (classify(test_data[index]))\n",
    "#     print('Identified ',identified_number, 'actually is', test_labels[index])\n",
    "    if(identified_number == test_labels[index]):\n",
    "        accuracy = accuracy + 1\n",
    "        \n",
    "print('Accuracy',(accuracy * 100) / 1000, '%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy using Bayesian pairwise majority voting method\n",
    "  \n",
    "import itertools\n",
    "all_numbers_data = {};\n",
    "# Calculate means for each class from data\n",
    "# now for pair (0,1) calculate cov matrix. and all inv, and constant term.\n",
    "# \n",
    "pairs =(list(itertools.combinations(range(10), 2)))\n",
    "def train_pairwise (train_data, train_labels):\n",
    "    mus = [];\n",
    "    n_features = len(train_labels)\n",
    "    for i in range(10):\n",
    "        train_sample = train_data[train_labels[:]==i]\n",
    "        mu = train_sample.mean ( axis=0 )\n",
    "        cov = np.cov(train_sample.T)\n",
    "        all_numbers_data[i]={\n",
    "            \"mu\":mu,\n",
    "            \"cov\":cov\n",
    "        }\n",
    "    for pair in pairs:\n",
    "        print('Training Pair', pair)\n",
    "        cov = 0.5* (all_numbers_data[pair[0]][\"cov\"] +  all_numbers_data[pair[1]][\"cov\"] )\n",
    "        mu_0 = all_numbers_data[pair[0]][\"mu\"]\n",
    "        mu_1 = all_numbers_data[pair[1]][\"mu\"]\n",
    "        inverse = getInverseFromCov(cov)\n",
    "            \n",
    "        # finding m and c\n",
    "        w = inverse.dot((mu_0 - mu_1))\n",
    "        x0 = 0.5*(mu_0 + mu_1)\n",
    "        m = w.T\n",
    "        c = w.T.dot(x0)        \n",
    "        dict_alldata[pair]={\n",
    "            \"m\":m,\n",
    "            \"c\":c\n",
    "        }\n",
    "    return dict_alldata\n",
    "\n",
    "\n",
    "def getInverseFromCov(cov):\n",
    "    eigenvalues, eigenvectors = LA.eigh(cov)\n",
    "    eigen = np.column_stack((eigenvalues,eigenvectors))\n",
    "    eigen = eigen[(-eigen[:,0]).argsort()]\n",
    "    sortedEigenValues = np.array(eigen[:,0])\n",
    "    sortedEigenVectors = np.array(eigen[:,1:])\n",
    "    approx_rank_k = len((np.where(sortedEigenValues>0))[0])\n",
    "    first_k_eigen_values = sortedEigenValues[:approx_rank_k]\n",
    "    first_k_eigen_values_inv = np.reciprocal(first_k_eigen_values)\n",
    "    first_k_eigen_vectors = sortedEigenVectors[:approx_rank_k,:]\n",
    "    inverse = first_k_eigen_vectors.T.dot(diag(first_k_eigen_values_inv)).dot(first_k_eigen_vectors)\n",
    "    return (inverse)\n",
    "    \n",
    "dict_alldata = train_pairwise(train_data,train_labels)\n",
    "\n",
    "def predict_classwise(x_test):\n",
    "    votes={}\n",
    "    for i in range(10):\n",
    "        votes[i]=0\n",
    "    \n",
    "    for pair in pairs:\n",
    "            m = dict_alldata[pair][\"m\"]\n",
    "            c = dict_alldata[pair][\"c\"]\n",
    "#             print('c', c)\n",
    "            line = m.dot(x_test) - c\n",
    "#             print('line', line)    \n",
    "            if( line > 0):\n",
    "                votes[pair[0]] = votes[pair[0]] + 1\n",
    "            else:\n",
    "                votes[pair[1]] = votes[pair[1]] + 1\n",
    "    maximum_votes_number = max(votes, key=votes.get)\n",
    "    return maximum_votes_number\n",
    "\n",
    "        \n",
    "accuracy = 0;\n",
    "for index in range(len(test_data)):\n",
    "    identified_number = (predict_classwise(test_data[index]))\n",
    "#     print('Identified ',identified_number, 'actually is', test_labels[index])\n",
    "    if(identified_number == test_labels[index]):\n",
    "        accuracy = accuracy + 1\n",
    "print('Accuracy is',(accuracy*100/1000), ' % ')\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy using Simple Perpendicular Bisector majority voting method\n",
    "# Print accuracy using Bayesian pairwise majority voting method\n",
    "  \n",
    "import itertools\n",
    "all_numbers_data = {};\n",
    "# Calculate means for each class from data\n",
    "# now for pair (0,1) calculate cov matrix. and all inv, and constant term.\n",
    "# \n",
    "pairs =(list(itertools.combinations(range(10), 2)))\n",
    "def train_pairwise (train_data, train_labels):\n",
    "    mus = [];\n",
    "    n_features = len(train_labels)\n",
    "    for i in range(10):\n",
    "        train_sample = train_data[train_labels[:]==i]\n",
    "        mu = train_sample.mean ( axis=0 )\n",
    "        cov = np.cov(train_sample.T)\n",
    "        all_numbers_data[i]={\n",
    "            \"mu\":mu,\n",
    "            \"cov\":cov\n",
    "        }\n",
    "    for pair in pairs:\n",
    "        print('Training Pair : ' ,pair)\n",
    "        cov = 0.5* (all_numbers_data[pair[0]][\"cov\"] +  all_numbers_data[pair[1]][\"cov\"] )\n",
    "        mu_0 = all_numbers_data[pair[0]][\"mu\"]\n",
    "        mu_1 = all_numbers_data[pair[1]][\"mu\"]\n",
    "        inverse = getInverseFromCov(cov)\n",
    "        # basically wT (x-x0) = 0 the perpendicular line. w is mu0-mu1. and x-x0 is the vector of line perpendicular\n",
    "        # its the same as question 3\n",
    "        w=mu_0 - mu_1\n",
    "        x0 = 0.5*(mu_0+mu_1)\n",
    "        c = w.T.dot(x0)\n",
    "        m = w.T    \n",
    "        \n",
    "        dict_alldata[pair]={\n",
    "            \"m\":m,\n",
    "            \"c\":c\n",
    "        }\n",
    "    return dict_alldata\n",
    "\n",
    "\n",
    "def getInverseFromCov(cov):\n",
    "    eigenvalues, eigenvectors = LA.eigh(cov)\n",
    "    eigen = np.column_stack((eigenvalues,eigenvectors))\n",
    "    eigen = eigen[(-eigen[:,0]).argsort()]\n",
    "    sortedEigenValues = np.array(eigen[:,0])\n",
    "    sortedEigenVectors = np.array(eigen[:,1:])\n",
    "    approx_rank_k = len((np.where(sortedEigenValues>0))[0])\n",
    "    first_k_eigen_values = sortedEigenValues[:approx_rank_k]\n",
    "    first_k_eigen_values_inv = np.reciprocal(first_k_eigen_values)\n",
    "    first_k_eigen_vectors = sortedEigenVectors[:approx_rank_k,:]\n",
    "    inverse = first_k_eigen_vectors.T.dot(diag(first_k_eigen_values_inv)).dot(first_k_eigen_vectors)\n",
    "    return (inverse)\n",
    "    \n",
    "dict_alldata = train_pairwise(train_data,train_labels)\n",
    "\n",
    "def predict_classwise(x_test):\n",
    "    votes={}\n",
    "    for i in range(10):\n",
    "        votes[i]=0\n",
    "    \n",
    "    for pair in pairs:\n",
    "            m = dict_alldata[pair][\"m\"]\n",
    "            c = dict_alldata[pair][\"c\"]\n",
    "#             print('c', c)\n",
    "            line = m.dot(x_test) - c\n",
    "#             print('line', line)    \n",
    "            if( line > 0):\n",
    "                votes[pair[0]] = votes[pair[0]] + 1\n",
    "            else:\n",
    "                votes[pair[1]] = votes[pair[1]] + 1\n",
    "    maximum_votes_number = max(votes, key=votes.get)\n",
    "    return maximum_votes_number\n",
    "\n",
    "        \n",
    "accuracy = 0;\n",
    "for index in range(len(test_data)):\n",
    "    identified_number = (predict_classwise(test_data[index]))\n",
    "#     print('Identified ',identified_number, 'actually is', test_labels[index])\n",
    "    if(identified_number == test_labels[index]):\n",
    "        accuracy = accuracy + 1\n",
    "print('Accuracy',accuracy*100/1000)\n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.3 Question 4\n",
    "Compare performances and salient observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Performances:\n",
    "1) MLE => Accuracy came out to be 10%. It identified all samples as 5. on doing MLE paramenters optimisation we get that mean is the mean of training data, and covaraiance is the covaraiance of the training data. \n",
    "2) MAP =>\n",
    "MAP and MLE will be same if we consider equal probability for each class.\n",
    "Depending on the prior for MAP, our result will vary\n",
    "\n",
    "3) Pairwise Bayesian => 77%.\n",
    "4) Perpendicular Line => maximum accuracy : 77%.\n",
    "3 and 4 are same, because we assume probability of each class is same, and the covariance is same, so the linear boundary comes out to be a line perpendicular to line joining mean and passing through mean\n",
    "Because for a pair, the covariance matrix is same, just a different mean, so the shape of gaussian is same, it is just shifted according to mean, and hence, line bisecting the mean, gives us our linear boundary\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## 1.3.4 Nearest Neighbour based Tasks and Design\n",
    "---\n",
    "### 1.3.4 Question 1 : NN Classification with various K\n",
    "Implement a KNN classifier and print accuracies on the test set with K=1,3,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Print accuracies with K = 1, 3, 7\n",
    "import operator\n",
    "\n",
    "def euclideanDistance(data1, data2, length):\n",
    "    distance = 0\n",
    "    for x in range(length):\n",
    "        distance += np.square(data1[x] - data2[x])\n",
    "    return np.sqrt(distance)\n",
    "\n",
    "\n",
    "# Defining our KNN model\n",
    "def knn(trainingSet, testInstance, k):\n",
    "    distances = {}\n",
    "    sort = {}\n",
    "    length = len(testInstance)\n",
    "    # Calculating euclidean distance between each row of training data and test data\n",
    "    for x in range(len(trainingSet)):\n",
    "        dist = euclideanDistance(testInstance, trainingSet[x], length)\n",
    "        distances[x] = dist\n",
    "        \n",
    "    #### Start of STEP 3.2\n",
    "    # Sorting them on the basis of distance\n",
    "    sorted_d = sorted(distances.items(), key=operator.itemgetter(1))\n",
    "    #### End of STEP 3.2\n",
    "    neighbors = []\n",
    "    \n",
    "    #### Start of STEP 3.3\n",
    "    # Extracting top k neighbors\n",
    "    for x in range(k):\n",
    "        neighbors.append(sorted_d[x][0])\n",
    "    classVotes = {}\n",
    "    \n",
    "    for x in range(len(neighbors)):\n",
    "        response = train_labels[neighbors[x]]\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1\n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return(sortedVotes[0][0], neighbors)\n",
    "\n",
    "def calculateAccuracy(k):\n",
    "    accuracy = 0;\n",
    "    test = test_data.T\n",
    "    train = train_data.T\n",
    "    for x in range(len(test)):\n",
    "        result,neigh = knn(train, test[x], k)\n",
    "        print('Predicted result', result, 'Actually it is ', test_labels[x])\n",
    "        accuracy = accuracy + (result == test_labels[x])\n",
    "    print('accuracy for k ::' ,k , ' : ', accuracy*100/1000)\n",
    "\n",
    "calculateAccuracy(1)\n",
    "calculateAccuracy(3)\n",
    "calculateAccuracy(7)\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# #Import scikit-learn metrics module for accuracy calculation\n",
    "# from sklearn import metrics\n",
    "\n",
    "\n",
    "# #Create KNN Classifier K=1\n",
    "# knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "# #Train the model using the training sets\n",
    "# knn1.fit(train_data, train_labels)\n",
    "# #Predict the response for test dataset\n",
    "# label_pred = knn1.predict(test_data)\n",
    "# print(label_pred.shape)\n",
    "# print(\"Accuracy for k=1:\",metrics.accuracy_score(test_labels, label_pred))\n",
    "\n",
    "\n",
    "# #Create KNN Classifier K=3\n",
    "# knn3 = KNeighborsClassifier(n_neighbors=3)\n",
    "# #Train the model using the training sets\n",
    "# knn3.fit(train_data, train_labels)\n",
    "# #Predict the response for test dataset\n",
    "# label_pred = knn3.predict(test_data)\n",
    "# print(label_pred.shape)\n",
    "# print(\"Accuracy for k=3:\",metrics.accuracy_score(test_labels, label_pred))\n",
    "\n",
    "# #Create KNN Classifier K=7\n",
    "# knn7 = KNeighborsClassifier(n_neighbors=7)\n",
    "# #Train the model using the training sets\n",
    "# knn7.fit(train_data, train_labels)\n",
    "# #Predict the response for test dataset\n",
    "# label_pred = knn7.predict(test_data)\n",
    "# print(label_pred.shape)\n",
    "# print(\"Accuracy for k=7:\",metrics.accuracy_score(test_labels, label_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.4 Question 1 continued\n",
    "- Why / why not are the accuracies the same?\n",
    "- How do we identify the best K? Suggest a computational procedure with a logical explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Accuracies\n",
    "k=1 is different 90%. k=3 and k=7 are same : 91%. \n",
    "\n",
    "for k value to be 1 , is less, for any conclusion. We can have outliers or anything that can change the result\n",
    "for k value to be 3 and 7, they are giving the same accuracy. But 7 increases computation.\n",
    "Small value of k means that noise will have a higher influence on the result nd a large value make it computationally expensive. \n",
    "\n",
    "\n",
    "Alogrithm:\n",
    "We can chose k to be sqrt(n) where n is no of classes. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.4 Question 2 :  Reverse NN based outlier detection\n",
    "A sample can be thought of as an outlier is it is NOT in the nearest neighbour set of anybody else. Expand this idea into an algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# This cell reads mixed data containing both MNIST digits and English characters.\n",
    "# The labels for this mixed data are random and are hence ignored.\n",
    "mixed_data, _ = read_data(\"outliers.csv\")\n",
    "print(mixed_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.4 Question 3 : NN for regression\n",
    "Assume that each classID in the train set corresponds to a neatness score as:\n",
    "$$ neatness = \\frac{classID}{10} $$\n",
    "\n",
    "---\n",
    "Assume we had to predict the neatness score for each test sample using NN based techiniques on the train set. Describe the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "yes we can use.\n",
    "Simply use the NN to get the class, and then divide by 10.\n",
    "\n",
    "Algorithm: keep k = 3 (close to sqrt(10))\n",
    "-> For each data in test_data\n",
    "\n",
    "-> compute distance with each of the training set\n",
    "\n",
    "-> now sort and find out top minimum three distance\n",
    "\n",
    "-> identify the class and now just think it by 10.\n",
    "\n",
    "-> and we have our neatness score\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.4 Question 3 continued\n",
    "Validate your algorithm on the test set. This code should print mean absolute error on the test set, using the train set for NN based regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Print accuracies with K = 1, 3, 7\n",
    "mean = np.mean(test_labels)\n",
    "print('Mean is ', mean)\n",
    "import operator\n",
    "\n",
    "def euclideanDistance(data1, data2, length):\n",
    "    distance = 0\n",
    "    for x in range(length):\n",
    "        distance += np.square(data1[x] - data2[x])\n",
    "    return np.sqrt(distance)\n",
    "\n",
    "\n",
    "# Defining our KNN model\n",
    "def knn(trainingSet, testInstance, k):\n",
    "    distances = {}\n",
    "    sort = {}\n",
    "    length = len(testInstance)\n",
    "    # Calculating euclidean distance between each row of training data and test data\n",
    "    for x in range(len(trainingSet)):\n",
    "        dist = euclideanDistance(testInstance, trainingSet[x], length)\n",
    "        distances[x] = dist\n",
    "        \n",
    "    #### Start of STEP 3.2\n",
    "    # Sorting them on the basis of distance\n",
    "    sorted_d = sorted(distances.items(), key=operator.itemgetter(1))\n",
    "    #### End of STEP 3.2\n",
    "    neighbors = []\n",
    "    \n",
    "    #### Start of STEP 3.3\n",
    "    # Extracting top k neighbors\n",
    "    for x in range(k):\n",
    "        neighbors.append(sorted_d[x][0])\n",
    "    classVotes = {}\n",
    "    \n",
    "    for x in range(len(neighbors)):\n",
    "        response = train_labels[neighbors[x]]\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1\n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return(sortedVotes[0][0], neighbors)\n",
    "\n",
    "\n",
    "def calculateMAE(k):\n",
    "    mae = 0;\n",
    "    test = test_data.T\n",
    "    train = train_data.T\n",
    "    for x in range(len(test)):\n",
    "        result,neigh = knn(train, test[x], k)\n",
    "        resultvalue = result/10\n",
    "        acutalvalue = test_labels[x]\n",
    "        mae = mae +  np.absolute(resultvalue - acutalvalue)\n",
    "        print('current mae...', mae)\n",
    "        \n",
    "    mae = mae / (len(test))\n",
    "    \n",
    "    print('MAE for', k , ' is : ', mae)\n",
    "    \n",
    "calculateMAE(3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "---\n",
    "# FOLLOW THE SUBMISSION INSTRUCTIONS\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
